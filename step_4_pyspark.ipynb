{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d8783e",
   "metadata": {},
   "source": [
    "**Unit 4 Deliverable 1**\n",
    "# Big Data Wrangling with Google nGrams (Step 4)\n",
    "**Author:** Raghad Ibrahim  \n",
    "**Date:** April 30, 2024  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108e3723-e0c9-427e-a7d5-a6490600b902",
   "metadata": {},
   "source": [
    "> ## Table of Contents:\n",
    ">> [Initialization](#1)\n",
    ">>\n",
    ">> [Installing and Importing Necessary Packages](#2)\n",
    ">>\n",
    ">> [A First Look At The Data](#3)\n",
    ">>\n",
    ">> [Spark SQL](#4)\n",
    ">>\n",
    ">> [Writing to HDFS](#5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f785f7b9-5a0b-40a8-b81c-0a9e44d5a56c",
   "metadata": {},
   "source": [
    "## Initialization <a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422dd60a",
   "metadata": {},
   "source": [
    "First I will initialize the `spark` application - make sure you have the `PySpark` environment selected, otherwise the Spark session need to be initialized manually.\n",
    "\n",
    "Once the session is initialized, we can may proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf18c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1714468804888_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-41-102.us-east-2.compute.internal:20888/proxy/application_1714468804888_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-44-161.us-east-2.compute.internal:8042/node/containerlogs/container_1714468804888_0003_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f55959ef650>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "077b0df4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1714468804888_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-41-102.us-east-2.compute.internal:20888/proxy/application_1714468804888_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-44-187.us-east-2.compute.internal:8042/node/containerlogs/container_1714468804888_0004_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1714468804888_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-41-102.us-east-2.compute.internal:20888/proxy/application_1714468804888_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-44-187.us-east-2.compute.internal:8042/node/containerlogs/container_1714468804888_0004_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.pyspark.python\": \"python3\",\n",
    "        \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "        \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "        \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb84218e-b22d-4086-94cc-62f829532b51",
   "metadata": {},
   "source": [
    "## Installing and Importing Necessary Packages <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05710613",
   "metadata": {},
   "source": [
    "We can also check the available python packages and potentially add new ones. First let's check what packages we do have using `sc.list_packages()` - we want to make sure that `pandas` and `matplotlib` are both available for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4793b0a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                    Version\n",
      "-------------------------- ----------\n",
      "aws-cfn-bootstrap          2.0\n",
      "beautifulsoup4             4.9.3\n",
      "boto                       2.49.0\n",
      "click                      8.1.3\n",
      "docutils                   0.14\n",
      "jmespath                   1.0.1\n",
      "joblib                     1.2.0\n",
      "lockfile                   0.11.0\n",
      "lxml                       4.9.2\n",
      "mysqlclient                1.4.2\n",
      "nltk                       3.8\n",
      "nose                       1.3.4\n",
      "numpy                      1.20.0\n",
      "pip                        20.2.2\n",
      "py-dateutil                2.2\n",
      "pystache                   0.5.4\n",
      "python-daemon              2.2.3\n",
      "python37-sagemaker-pyspark 1.4.2\n",
      "pytz                       2022.7\n",
      "PyYAML                     5.4.1\n",
      "regex                      2021.11.10\n",
      "setuptools                 28.8.0\n",
      "simplejson                 3.2.0\n",
      "six                        1.13.0\n",
      "tqdm                       4.64.1\n",
      "wheel                      0.29.0\n",
      "windmill                   1.6\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag."
     ]
    }
   ],
   "source": [
    "sc.list_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af76a733-40d8-4154-ac42-4db5cdfb0a87",
   "metadata": {},
   "source": [
    "Seems like `pandas` and `matplotlib` are nowhere to be seen so we can proceed with installing them as demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0654fbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.0.5\n",
      "  Downloading pandas-1.0.5-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Collecting python-dateutil>=2.6.1\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas==1.0.5) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.7/site-packages (from pandas==1.0.5) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==1.0.5) (1.13.0)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-1.0.5 python-dateutil-2.9.0.post0\n",
      "\n",
      "Collecting matplotlib==3.1.1\n",
      "  Downloading matplotlib-3.1.1-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib64/python3.7/site-packages (from matplotlib==3.1.1) (1.20.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in ./tmp/1714475781844-0/lib/python3.7/site-packages (from matplotlib==3.1.1) (2.9.0.post0)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib==3.1.1) (1.13.0)\n",
      "Collecting typing-extensions; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: cycler, pyparsing, typing-extensions, kiwisolver, matplotlib\n",
      "Successfully installed cycler-0.11.0 kiwisolver-1.4.5 matplotlib-3.1.1 pyparsing-3.1.2 typing-extensions-4.7.1\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag."
     ]
    }
   ],
   "source": [
    "# install data science & plotting packages\n",
    "\n",
    "sc.install_pypi_package(\"pandas == 1.0.5\") \n",
    "sc.install_pypi_package(\"matplotlib == 3.1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34edadfd-fb3f-466a-a06e-857162dc07c5",
   "metadata": {},
   "source": [
    "Now that packages are installed, we can import them into the notebook as per usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46c9a2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8655fd8a-6f68-4ee3-a1ff-e8cf525a009f",
   "metadata": {},
   "source": [
    "## A First Look At The Data  <a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d483c9",
   "metadata": {},
   "source": [
    "Spark can read data locally from files, databases, large datasets from HDFS, or even from S3 buckets! In this case we will be reading the data from HDFS where we have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7aa5a7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read data as dataframe\n",
    "\n",
    "df = spark.read.csv('/user/hadoop/eng_1M_1gram', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca2aeac",
   "metadata": {},
   "source": [
    "We can print out the schema and data types in human-readable format using `printSchema()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "317031dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- token: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- frequency: string (nullable = true)\n",
      " |-- pages: string (nullable = true)\n",
      " |-- books: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "# print out schema and data types\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48a31f-7be0-4a08-bf2d-74d6cddff23e",
   "metadata": {},
   "source": [
    "And now let's take a first look at the data. Or more specifically, the top $10$ rows of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d15c714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+-----+-----+\n",
      "|    token|year|frequency|pages|books|\n",
      "+---------+----+---------+-----+-----+\n",
      "|inGermany|1927|        2|    2|    2|\n",
      "|inGermany|1929|        1|    1|    1|\n",
      "|inGermany|1930|        1|    1|    1|\n",
      "|inGermany|1933|        1|    1|    1|\n",
      "|inGermany|1934|        1|    1|    1|\n",
      "|inGermany|1935|        1|    1|    1|\n",
      "|inGermany|1938|        5|    5|    5|\n",
      "|inGermany|1939|        1|    1|    1|\n",
      "|inGermany|1940|        1|    1|    1|\n",
      "|inGermany|1942|        2|    2|    2|\n",
      "+---------+----+---------+-----+-----+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# show top 10 rows\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d08cb64-1330-4ffb-8652-757e7843dc52",
   "metadata": {},
   "source": [
    "It is apparent that the dataset has $5$ columns. To check the number of rows in this dataset, we can use `df.count()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b302f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count() # i accidentally reran it after exiting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c27177d-8500-4b45-8e69-c42cbc813aa7",
   "metadata": {},
   "source": [
    "**This dataset has $261,823,225$ rows!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0fc608-df69-480e-bef4-9384b0c75d9e",
   "metadata": {},
   "source": [
    "## Spark SQL  <a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27794af-4ae2-4c6b-861d-4170fa316968",
   "metadata": {},
   "source": [
    "Now we will create a new DataFrame from a query using Spark SQL, filtering to include only the rows where the token is \"data\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e42a9f",
   "metadata": {},
   "source": [
    "Using SQL requires the `CreateorReplaceTempView` function, which registers the data as a view in the Spark session. We can then query against that view with SQL, with the view name being the table name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c39f9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"temp_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9cf854-ee36-46d2-af1b-b0b47df2a38c",
   "metadata": {},
   "source": [
    "To display the first $5$ rows of the data, we can simply type out an SQL command like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9c99be3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+-----+-----+\n",
      "|    token|year|frequency|pages|books|\n",
      "+---------+----+---------+-----+-----+\n",
      "|inGermany|1927|        2|    2|    2|\n",
      "|inGermany|1929|        1|    1|    1|\n",
      "|inGermany|1930|        1|    1|    1|\n",
      "|inGermany|1933|        1|    1|    1|\n",
      "|inGermany|1934|        1|    1|    1|\n",
      "+---------+----+---------+-----+-----+"
     ]
    }
   ],
   "source": [
    "# query to display top 5 rows\n",
    "\n",
    "spark.sql(\"SELECT * FROM temp_df limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a912fa9-4743-4dec-8cf7-5c60fded4a93",
   "metadata": {},
   "source": [
    "We can also filter the rows with books that have the word \"data\" in them, i.e. `token == data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "000a77a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+-----+-----+\n",
      "|token|year|frequency|pages|books|\n",
      "+-----+----+---------+-----+-----+\n",
      "| data|1584|       16|   14|    1|\n",
      "| data|1614|        3|    2|    1|\n",
      "| data|1627|        1|    1|    1|\n",
      "| data|1631|       22|   18|    1|\n",
      "| data|1637|        1|    1|    1|\n",
      "| data|1638|        2|    2|    1|\n",
      "| data|1640|        1|    1|    1|\n",
      "| data|1642|        1|    1|    1|\n",
      "| data|1644|        4|    4|    1|\n",
      "| data|1647|        1|    1|    1|\n",
      "+-----+----+---------+-----+-----+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# query to filter the rows where token == data\n",
    "\n",
    "spark.sql(\"SELECT * FROM temp_df WHERE token == 'data';\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e1161-b4f2-4950-9dd4-c4b3fdc250db",
   "metadata": {},
   "source": [
    "For a statistical description of the dataset, we can use `.describe().show()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3b3d0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+-----------------+------------------+------------------+\n",
      "|summary|token|              year|        frequency|             pages|             books|\n",
      "+-------+-----+------------------+-----------------+------------------+------------------+\n",
      "|  count|  316|               316|              316|               316|               316|\n",
      "|   mean| null|1847.5696202531647|38555.99367088608|21711.041139240508| 1493.110759493671|\n",
      "| stddev| null| 96.87438222401165| 69212.3664179185| 34901.79774004759|1560.0408024002788|\n",
      "|    min| data|              1584|                1|                 1|                 1|\n",
      "|    max| data|              2008|            98764|             99110|               955|\n",
      "+-------+-----+------------------+-----------------+------------------+------------------+"
     ]
    }
   ],
   "source": [
    "# describing the rows where token is equal to data\n",
    "\n",
    "spark.sql(\"SELECT * FROM temp_df WHERE token == 'data';\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e230f-34f5-499c-8e3d-131c5b958fc0",
   "metadata": {},
   "source": [
    "We have $316$ rows which represents the number of distinct years in which books with the word \"data\" were published. The `min` and `max` for the `year` column are $1584$ and $2008$ respectively, in other words the word \"data\" first appears in books published in the year $1584$. The highest number of occurrences of the word in a single year was $98,764$ times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787992ff-db44-403f-b215-f556b1f98dc1",
   "metadata": {},
   "source": [
    "Now that we have filtered the rows where token is equal to data, let us save it into new dataframe so that we don't have to filter it again. The reason we pass the `header = True` parameter is to prevent a duplicate header. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf905744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# saving in a dataframe\n",
    "\n",
    "filtered_data=spark.sql(\"SELECT * FROM temp_df WHERE token == 'data';\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c75651-e916-44b9-ae0b-63b72958a824",
   "metadata": {},
   "source": [
    "## Writing to HDFS <a class=\"anchor\" id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5fabb5-4602-4409-a81b-fb6a6167f1eb",
   "metadata": {},
   "source": [
    "Finally, we will write the filtered data back to a directory in the HDFS from Spark using `df.write.csv()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9df6dda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# writing to HDFS using `write.csv()`\n",
    "\n",
    "filtered_data.write.csv('/user/hadoop/filtered_data', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962aa4fe-a620-4808-bd8b-9d4b96604b45",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e70616-87c2-4aed-9e9e-d14a54c4d387",
   "metadata": {},
   "source": [
    "Refer to the report for the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3a8ad1-3c7a-4eac-bd0a-f0f015c0bd91",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
